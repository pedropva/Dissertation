%%% -*- coding: utf-8 -*-
\newpage

\chapter{Related Work}
\label{chap:related}

 %O dataset que mais se aproximou com a nossa definição foi o MediaEval 2015, porém nosso dataset vem de principalmente de gravações amadoras ou CCTV, não de cenas cinematográficas.
 
% commented out because wqe have a more recent paper from theese same authors
%Song and Kin~\cite{song2018pornographic} create a scheme for detecting pornography videos using multimodal features: Image descriptor features of the frame sequence, extracted using the VGG-16 CNN\cite{vgg}, motion features extracted using optical flow\cite{opticalflow}, and audio features extracted using a Mel-scaled spectrogram.
%The final features for each model are obtained by an average pooling of each of the features by a sample in the video.
%Each of those kinds of features is used in a single SVM classifier per type of feature, resulting in an image sequence-based detector, a motion-based detector, and an audio-based detector.
%The final decision-making is done by model stacking all detectors.
%The authors used a modified dataset based on the 2k-pornography dataset\cite{2kdataset} for training and testing.
%The results of their method are an average of 63.4\% with a 100\% true positive rate for porn, and an average of 23.5\% of the false-positive rate. 
%Our work also uses multimodal features, but we only use image sequence features and audio features, furthermore, we use an Inception-V3 CNN instead of the VGG-16 CNN for extracting image sequence features and use an Audio VGG CNN for extracting audio features.
%Although the authors achieve 100\% recall rate for pornography, which is the main goal of their task, their model also has a 23.5\% false-positive rate, which means that normal videos would be occasionally classified as pornography.
%Our aim is to also have a true positive rate as high as theirs, while still further reducing the false positive rate.

Castro~\cite{torres2018automatic} shows an implementation of a pornography video classifier using a convolutional neural network from Open pornography~\cite{mahadeokar2016open} and the dataset from Nude Detection in Video using Bag-of-Visual-Features~\cite{lopes2009nude} dataset.
The CNN does a logistic regression on each frame, resulting in a value from 0 to 1 at each frame.
The higher the value is, the higher is the likelihood of the frame being pornography.
The dataset used contained 90 non-pornography video segments and 89 pornography video segments extracted from 11 movies.
The final score for the video the max value from all frames of the video.
The experiment showed an accuracy of 81\%, and a F1-score, and Matthew’s correlation coefficient(MCC) for the pornography class of 0.8047 and 0.6343, respectively.
Although the work also approaches pornography content detection in videos problem with CNN like ours, it does not make use of audio features.
The method is also different, it performs the regression first, then it takes the max value from all frames of the video, while ours, in the non-sequential approach, combines features from all frames of the video into a single vector of features (mainly by averaging) and then performs classification on the resulting features.

% Tem o trabalho do professor novo também, ele usa convnets pra extrair as features e uma rnn pra classificar
%https://www.sciencedirect.com/science/article/pii/S0925231217312493
Wehrmann \textit{et al.}~\cite{wehrmann2018adult} classify adult content trained on the NPDI pornography video dataset ~\cite{avila2013pooling}, which consists of 802 videos, totaling 80 hours of videos, half of them with adult content.
Those videos were processed by keyframes, varying between 1 and 320 frames per video.
The selected keyframes of each video were chosen by a scene segmentation algorithm, resulting in 16727 images. % this was done by the dataset authors, not the work authors
Their architecture consists of a Convolutional Network and an Long-Short Term Memory Network (LSTM)~\cite{hochreiter1997long}.
Those models were chosen for feature extraction with CNN and sequence learning with LSTM, taking into consideration modifications on the images such as scaling and distorting.
Using this approach the authors achieved a score of 95.6\% ± 1 accuracy and 0.990 AU(ROC).
In our model, we also approached the video analysis using frame by frame processing, but but we also processed the extracted sound from each frame.% using a pre-trained Convolutional Neural Network%, instead of an untrained one.
%\hl{Resulting in an accuracy of 98\% and 97.97\% F1-score for pornography class.}

Sing \textit{et. al.}\cite{singh2019kidsguard} proposes a fine-grained approach for child unsafe video representation and detection. One of its main objectives is to optimize detection on sparsely present child unsafe content and it does so by using a VGG16\cite{vgg} Convolutional Neural Network (CNN) to encode each frame, at 1-second granularity, in 512 real values. 
Then an LSTM autoencoder is trained to output the sequence backward on those encoded frames. 
Once the LSTM autoencoder is trained, then a fully connected layer of neurons is used to fine-tune and classify each frame. 
The dataset used comprises 109,835 short-duration video clips extracted from four animes. 
The results for binary classification using safe and unsafe classes were 81\% recall for unsafe and 0.88 AUC(ROC) for unsafe class.
Although this work also has similar objectives as ours and also uses a CNN-based encoding method, ours uses both visual and audio features to encode a video. 
%Their work uses 1 frame per second granularity and ours has the same encoding rate. 
The main difference between both works is on the dataset: Theirs consists of small clips of only anime videos. Ours also uses other types of videos such as live-action and other animations. 
%In our dataset, the length of videos range from 6 seconds to 30 minutes.

Song \textit{et. al.}~\cite{song2020enhanced} proposed a multimodal stacking scheme for fast and accurate online detection of pornographic content.
Their work uses both visual and auditory features as input for their detection method. 
They use a VGG16 model and a bi-directional LSTM to extract visual features and a combination of a Mel-scaled spectrogram followed by multilayered dilated convolutions to extract audio features. 
Using only the visual and auditory features, a video classifier and an audio classifier are trained, respectively. 
By using both features together, one fusion classifier is also trained.
Then, these three component classifiers are combined in an ensemble scheme to reduce the false-negative errors and for faster detection. 
The proposed detection method yields a true positive rate of 95.40\% and a false negative rate of 4.60\% on the pornography class, totaling a recall for the pornography class of 95.40\% and accuracy of 92.33\%. 
The dataset used was the NPDI 2k-pornography\cite{2kdataset} dataset plus examples of videos with only pornographic or non-pornographic audio collected by the authors. 
This work is similar to ours because it also uses a multimodal approach to detection, albeit ours is not for pornography detection only.
It also uses the same sampling rate of a frame for each second and uses a deep learning method for extracting high-level features, which are then classified by one or more machine learning models. 
% Our work has a different dataset, comprised of pornography, gore, and violent videos for the inappropriate class and miscellaneous and educational YouTube videos as an appropriate class. 
We also use different feature extraction methods for image and audio features. 
Finally, in contrast with their ensemble approach, we use a single model to classify the extracted features from our dataset.


Moreira \textit{et.al.}~\cite{moreira2019multimodal} has similar detection focuses as ours: Pornography and Violence. 
Their method uses four multi-modal classifiers, two for audio and two for image, those classifiers were fed features from multiple handcrafted feature extraction methods. Their work is geared towards mobile device applications and also allows for sensitive scene localization.
The authors propose a method for sensitive scene localization which uses the output of four multi-modal classifiers on snippets of the video, then creates a fusion vector at each second of the video. 
Finally, they test different classifiers on the fusion vector for each task: detecting pornography and detecting violence. Their best result on the pornography task was 90.75\% accuracy and 93.53\% on the F2 metric. For the violent videos, they achieved 0.502 on the MAP2014 evaluation metric.
Some differences between this work and our are mainly its objectives: To detect if and at what time the sensitive video occurs. While our only objective is to detect if there is or not sensitive content in a video. Their method is geared towards mobile devices, while ours is geared towards video hosting platforms.
%Our definition of violent videos consists of extremely violent videos, while theirs also considers videos with minor violence such as fights.
Other differences stand out as the dataset and the methods used for feature extraction and classification. 
The Violent Scenes Dataset \cite{VSD2014} is comprised of violent scenes from movies, while ours contains real violent scenes.
We use an authorial dataset and investigate what results a deep learning-based approach to this problem can yield.


%THEIR METHOD EVALUATES ON EACH FRAME, OUR ON THE ENTIRE VIDEO}

Wang \textit{et. al.} ~\cite{wang2019porn} proposes a pornography method for use in live streams, focusing on real-time processing, their work uses multimodal features, namely, image, audio, and optical flow~\cite{horn1981opticalflow}. 
An Xception~\cite{chollet2017xception} model is used to extract spatial features from keyframes. 
To get the optical flow frames, they also use a CNN to extract the optical flow from the video, then, use another Xception model to extract the high-level optical flow features. 
Finally, they use a short-time Fourier transformation to create spectrograms and feed those spectrograms to a third Xception model and thus acquiring the extracted audio features.  
Each of the multimodal features extracted then is passed onto bidirectional GRUs\cite{dey2017GRU}, to obtain temporal context, then, to create a better-unified representation, all the features go through three interconnected Attention-gated layers, each with three Attention-gated units proposed in the paper. After obtaining the dense representation of the input types, it is applied a fully connected layer of neurons with \textit{softmax} function. Their work archives 76.33\% accuracy and runs at 66.1 fps.
In our work, we strive for detecting both violence and pornography, we use only two types of input data, image, and audio, and we use a specific CNN for each type of data, while their work focused only on detecting pornography and used the same CNN model for all three types of input. 
%We investigate whether bigger and more specialized models can create better high-level features and further increase the quality of classification.

Liu \textit{et. al.} ~\cite{liu2020analyzing} proposes a multi-modal approach to pornography detection, it uses audio-frames and visual-frames to create handcrafted low-level features based on, respectively, periodic patterns and salient regions. Once those features are extracted, they use k-means clustering to create audio and visual codebooks. 
Then, low-level audio and visual features of test videos are converted into mid-level semantic histograms via de audio or visual codebook. 
Finally, the histograms are concatenated to represent the video and a periodicity-based video decision algorithm is used to fuse the classification results of multi-modal codebooks and the results of an SVM trained on the concatenated mid-level semantic features train set.
The true positive rate of their approach achieves 96.7\% while the false positive rate is about 10\%.
%Liu \textit{et. al.} detects pornography, they do not detect violence, and also use handcrafted features such as Region Of Interest (ROI) extraction and skin-color segmentation.
%Whereas our approach uses a fully automatic feature extraction method based on CNNs and our feature fusion method consists of just a concatenation followed by a classifier.

Most related works focus on pornography detection alone, while ours aims at detecting either pornographic or violent content. Moreover, some of them only use image-frame features, whereas we use both audio and image-frame features. We also use deep learning feature extraction methods instead of hand-crafted ones. Feature extraction method, classification method and dataset of each related work are available in Table \ref{tab:related}.
Finally, a central difference is our dataset: Ours contains violent scenes and is significantly larger than most datasets used on other related works.

\begin{landscape}
\begin{table}[]
\scriptsize
\centering
\caption{Related work comparative table.}
\begin{tabular}{l|l|l|l|l|l|l|l|l|l}
Paper                                & Task                                                                    & Feature extraction method                                                                                                                                           & Classifier                                                                                 & F2-Score                                                        & Recall  & Accuracy                                                                                 & F1-Score & AUC (ROC) & Dataset                                                                                                                             \\ \hline
Castro \cite{torres2018automatic}           & \begin{tabular}[c]{@{}l@{}}Pornography\\ detection\end{tabular}         & Resnet 50 for image.                                                                                                                                                & Resnet-50                                                                                  & 0,7798                                                          & 0,7640  & 0,8160                                                                                   & 0,8047   & NA        & \begin{tabular}[c]{@{}l@{}}Open pornography\\  + Nude Detection in \\ Video using \\ Bag-of-Visual-Features \\ dataset\end{tabular} \\ \hline
Wehrmann et al. \cite{wehrmann2018adult}    & \begin{tabular}[c]{@{}l@{}}Pornography\\ detection\end{tabular}         & ResNet-101 for image.                                                                                                                                               & LSTM                                                                                       & 0,9520                                                          & 0,9501  & 0,9560                                                                                   & 0,9548   & 0,9900    & \begin{tabular}[c]{@{}l@{}}NPDI pornography\\ video dataset\end{tabular}                                                            \\ \hline
Sing et. al. \cite{singh2019kidsguard}      & \begin{tabular}[c]{@{}l@{}}Sensitive\\ content\\ detection\end{tabular} & VGG16 for image.                                                                                                                                                    & \begin{tabular}[c]{@{}l@{}}LSTM \\ + FC\end{tabular}                                       & NA                                                              & 0,8100  & NA                                                                                       & NA       & 0,8800    & Author (Animes)                                                                                                                     \\ \hline
Song et. al. \cite{song2020enhanced}        & \begin{tabular}[c]{@{}l@{}}Pornography\\ detection\end{tabular}         & \begin{tabular}[c]{@{}l@{}}VGG16\\ + BiLSTM for image;\\ Mel-scaled spectrogram\\ + Multilayered dilated\\ convolutions for audio.\end{tabular}                     & \begin{tabular}[c]{@{}l@{}}Early \\ + Late fusion FC\\ voting\end{tabular}                 & NA                                                              & 95.40\% & 0,9233                                                                                   & NA       & NA        & \begin{tabular}[c]{@{}l@{}}NPDI\\ 2k-pornography\end{tabular}                                                                       \\ \hline
Moreira et.al. \cite{moreira2019multimodal} & \begin{tabular}[c]{@{}l@{}}Sensitive\\ content\\ detection\end{tabular} & \begin{tabular}[c]{@{}l@{}}HOG \\ for image;\\ TRoF \\ for space-temporal description;\\ MFCC and \\ prosodic features\\  for audio.\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}Thresholding \\ (Pornography);\\ SVM\\ (Violence).\end{tabular} & \begin{tabular}[c]{@{}l@{}}93,53\%\\ (Pornography)\end{tabular} & NA      & \begin{tabular}[c]{@{}l@{}}90,75\%\\(Pornography);\\0.502 MAP2014\\(Violence).\end{tabular} & NA       & NA        & \begin{tabular}[c]{@{}l@{}}NPDI\\ 2k-pornography +\\ Violent Scenes Dataset\\ (MediaEval 2014)\end{tabular}                         \\ \hline
Wang et al. \cite{wang2019porn}             & \begin{tabular}[c]{@{}l@{}}Pornography\\ detection\end{tabular}         & \begin{tabular}[c]{@{}l@{}}Xception for image;\\ Optical flow \\ + Xception for motion;\\ Short-time Fourier\\ transformation \\ + Xception for audio.\end{tabular} & \begin{tabular}[c]{@{}l@{}}bidirectional GRUs \\ + Attention \\ + FC\end{tabular}          & NA                                                              & NA      & 76,33\%                                                                                  & NA       & NA        & \begin{tabular}[c]{@{}l@{}}BJUT streamer\\  dataset (Author)\end{tabular}                                                           \\ \hline
Liu et. al. \cite{liu2020analyzing}         & \begin{tabular}[c]{@{}l@{}}Pornography\\ detection\end{tabular}         & \begin{tabular}[c]{@{}l@{}}Skin color detection \\ + Face detection \\ + Salient regions detection \\ + SURF for image.\end{tabular}                                & SVM                                                                                        & NA                                                              & NA      & 96,70\%                                                                                  & NA       & NA        & Author                                                                                                                             
\end{tabular}
\label{tab:related}
\end{table}
\end{landscape}